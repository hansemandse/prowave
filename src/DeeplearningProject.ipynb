{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProWave - WaveNet-based Protein Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Hans Jakob Damsgaard & Lucas Balling\n",
    "\n",
    "02456 Deep Learning project: ProGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the commmand below if you have not yet installed the [TAPE project](https://github.com/songlab-cal/tape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tape_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data\n",
    "\n",
    "We were unable to make the data download script, `download_data.sh`, run from Jupyter, so instead we ran it manually and simply placed the resulting files in the right folder for TAPE to find them. We import all the data in the LMDB format as it is most easily worked with in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.datasets import LanguageModelingDataset\n",
    "\n",
    "# Data stored under `<data-path>/data`\n",
    "#data_path = '/Users/lucasballing/Desktop/DeepLearningProject/prowave-main/data/'\n",
    "data_path = 'E:/Pfam/data/'\n",
    "train_data   = LanguageModelingDataset(data_path, 'train')\n",
    "valid_data   = LanguageModelingDataset(data_path, 'valid')\n",
    "holdout_data = LanguageModelingDataset(data_path, 'holdout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding data features\n",
    "\n",
    "To get a good understanding of the data provided in the imported dataset, we provide plots of certain features and their ranges. Data is already split into the three required subsets; train, validation, and holdout by TAPE, so it is also interesting to understand this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data has shape (32593668, 4)\nValidation data has shape (1715454, 4)\nHoldout data has shape (44311, 4)\nFile data entries look like this: {'primary': 'GCTVEDRCLIGMGAILLNGCVIGSGSLVAAGALITQ', 'protein_length': 36, 'clan': 433, 'family': 9122, 'id': '0'}\nEncoded data entries look like this: (array([ 2, 11,  7, 23, 25,  9,  8, 21,  7, 15, 13, 11, 16, 11,  5, 13, 15,\n       15, 17, 11,  7, 25, 13, 11, 22, 11, 22, 15, 25,  5,  5, 11,  5, 15,\n       13, 23, 20,  3], dtype=int64), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64), 433, 9122)\n"
     ]
    }
   ],
   "source": [
    "# Split sizes\n",
    "print(f'Training data has shape ({len(train_data)}, {len(train_data[0])})')\n",
    "print(f'Validation data has shape ({len(valid_data)}, {len(valid_data[0])})')\n",
    "print(f'Holdout data has shape ({len(holdout_data)}, {len(holdout_data[0])})')\n",
    "\n",
    "# Original data columns\n",
    "from tape.datasets import LMDBDataset\n",
    "lmdb_train = LMDBDataset(data_path+'pfam/pfam_train.lmdb')\n",
    "print(f'File data entries look like this: {lmdb_train[0]}')\n",
    "del lmdb_train\n",
    "\n",
    "# Data columns - all subsets are taken from the same overall dataset, so the columns are the same\n",
    "# From combining information from LMDBDataset and LanguageModelingDataset, we know the columns are\n",
    "# - IUPAC-encoded protein string\n",
    "# - Input mask (for masked-token prediction)\n",
    "# - Protein clan\n",
    "# - Protein family\n",
    "# The protein ID (i.e., its number within its clan and family) is not included\n",
    "print(f'Encoded data entries look like this: {train_data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setify(data):\n",
    "    \"\"\"\n",
    "    Produces summary statistics for the provided datasets.\n",
    "\n",
    "    Args:\n",
    "     `data`: a list of datasets\n",
    "\n",
    "    Returns a tuple of three lists;\n",
    "     `uniques` representing sets of clans and families in each of the datasets\n",
    "     `perclan` representing tuples of clan ID and protein count\n",
    "     `perfam`  representing tuples of family ID and protein count\n",
    "    \"\"\"\n",
    "    uniques = [[set() for _ in range(len(data))] for _ in range(2)]\n",
    "    perclan = [{} for _ in range(len(data))]\n",
    "    perfam  = [{} for _ in range(len(data))]\n",
    "    for index, d in enumerate(data):\n",
    "        for i in range(len(d)):\n",
    "            # Fetch this entry\n",
    "            row  = d[i]\n",
    "            clan = row[2]\n",
    "            fam  = row[3]\n",
    "\n",
    "            # Add clan and family IDs to sets\n",
    "            uniques[0][index].add(clan) # add clan\n",
    "            uniques[1][index].add(fam) # add family\n",
    "\n",
    "            # Count proteins in this clan\n",
    "            if clan not in perclan[index]:\n",
    "                perclan[index][clan] = 1\n",
    "            else:\n",
    "                perclan[index][clan] += 1\n",
    "\n",
    "            # Count proteins in this family\n",
    "            if fam not in perfam[index]:\n",
    "                perfam[index][fam] = 1\n",
    "            else:\n",
    "                perfam[index][fam] += 1\n",
    "\n",
    "    return uniques, [x.items() for x in perclan], [x.items() for x in perfam]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch results from all splits\n",
    "results = setify([train_data, valid_data, holdout_data])\n",
    "\n",
    "# Clans in splits\n",
    "clans = results[0][0]\n",
    "print(f'Unique clans in training data {len(clans[0])}')\n",
    "print(f'Unique clans in validation data {len(clans[1])}')\n",
    "print(f'Unique clans in holdout data {len(clans[2])}')\n",
    "\n",
    "# Families in splits\n",
    "families = results[0][1]\n",
    "print(f'Unique families in training data {len(families[0])}')\n",
    "print(f'Unique families in validation data {len(families[1])}')\n",
    "print(f'Unique families in holdout data {len(families[2])}')\n",
    "\n",
    "# PRINTS:\n",
    "# Unique clans in training data 623\n",
    "# Unique clans in validation data 623\n",
    "# Unique clans in holdout data 8\n",
    "# Unique families in training data 17737\n",
    "# Unique families in validation data 15974\n",
    "# Unique families in holdout data 28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating some histograms\n",
    "In this section, we plot some histograms of the datasets to visulise the distribution of clan and family IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of protein counts in clans\n",
    "# TRAINING\n",
    "df = pd.DataFrame(results[1][0], columns=['Clan', 'Count'])\n",
    "sns.displot(df, x='Clan')\n",
    "plt.title('Training - Protein count vs Clan')\n",
    "plt.show()\n",
    "\n",
    "# VALIDATION\n",
    "df = pd.DataFrame(results[1][1], columns=['Clan', 'Count'])\n",
    "sns.displot(df, x='Clan')\n",
    "plt.title('Validation - Protein count vs Clan')\n",
    "plt.show()\n",
    "\n",
    "# HOLDOUT\n",
    "df = pd.DataFrame(results[1][2], columns=['Clan', 'Count'])\n",
    "sns.displot(df, x='Clan')\n",
    "plt.title('Holdout - Protein count vs Clan')\n",
    "plt.show()\n",
    "\n",
    "# Histograms of protein counts in families\n",
    "# TRAINING\n",
    "df = pd.DataFrame(results[2][0], columns=['Family', 'Count'])\n",
    "sns.displot(df, x='Family')\n",
    "plt.title('Training - Protein count vs Clan')\n",
    "plt.show()\n",
    "\n",
    "# VALIDATION\n",
    "df = pd.DataFrame(results[2][1], columns=['Family', 'Count'])\n",
    "sns.displot(df, x='Family')\n",
    "plt.title('Validation - Protein count vs Clan')\n",
    "plt.show()\n",
    "\n",
    "# HOLDOUT\n",
    "df = pd.DataFrame(results[2][2], columns=['Family', 'Count'])\n",
    "sns.displot(df, x='Family')\n",
    "plt.title('Holdout - Protein count vs Clan')\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "# Idea and Initial Implementation\n",
    "We intend to follow the ideas presented in the ProGen paper relatively closely. That is, our network will be trained on conditioned aminoacid sequences with the only two available conditioning tags being the clan and family IDs. To enable this, our network's input will be as shown on the figure below.\n",
    "\n",
    "<img src=\"../Training.png\" width=\"500\"/>\n",
    "\n",
    "So, like ProGen, our input $x=[c;a]$ is a sequence of encoded conditioning tags $c=(c_0,...,c_n)$ (in this case just two), and a starting sequence $a=(a_0, ..., a_n)$ of aminoacids for starting the sequence generation. The sequence is fed one character at a time to the network, which accumulates state before starting generation. We intend to let the network run free either until it generates an end character or until the generated sequence has length 500.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "One way to represent a fixed amount of words is through one-hot encoding. We intend to use one-hot encoding for both aminoacids as well as conditioning tags meaning that the input characters to our network become rather large. This may seem inefficient due to the great sparsity in the vectors produced, but we intend to limit this sparsity by decreasing the number of clans and families considered. This will allow us to generate proteins only for a subset of the available clans and families but at a much shorter required training and evaluation time. \n",
    "\n",
    "An example of a one-hot encoding is shown below:\n",
    "\n",
    "| Amionacid    | one-hot encoded vector   |\n",
    "| ------------- |--------------------------|\n",
    "| Ala = A        | $= [1, 0, 0, \\ldots, 0]$ |\n",
    "| Asx = B        | $= [0, 1, 0, \\ldots, 0]$ |\n",
    "| Cys = C    | $= [0, 0, 1, \\ldots, 0]$ |\n",
    "| ... | ... |\n",
    "| pad   | $= [0, 0, 0, \\ldots, 1]$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is taken from EXE 5.1 RNN\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word, vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "# End of EXE 5.1 RNN\n",
    "\n",
    "# Testing the implementation\n",
    "vocab_size = 30\n",
    "test_word = one_hot_encode(1, vocab_size)\n",
    "print(f'Our one-hot encoding of \\'1\\' has shape {test_word.shape}.')\n",
    "print(test_word)\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(holdout_data[1][0], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'{holdout_data[1][0]}\\' has shape {test_sentence.shape}.')\n",
    "print(test_sentence)"
   ]
  },
  {
   "source": [
    "## Reducing input size\n",
    "To reduce training time and problem size, we intend to extract 10 clans and their families from the training set and use those for training the network. Data is stored in sorted order in the training data set meaning that we can simply store elements until we see the 11th clan ID at which we can stop looking through the dataset. This is preferable over having to look through the entire dataset (as we did to gather plot data earlier), as such an operation takes a long time due to the mere size of the dataset.\n",
    "\n",
    "As part of this operation, we also throw away the input masks that are included in the original dataset and attempt to limit data size by using `int8` instead of the original `int64` format for encoded sequences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, num):\n",
    "    \"\"\"\n",
    "    Fetches data for `num` clans in the given dataset\n",
    "\n",
    "    Args:\n",
    "     `dataset`: an LMDB dataset\n",
    "     `num`: a number of clans to collect data for\n",
    "\n",
    "    Returns a DataFrame object with the fetched data\n",
    "    \"\"\"\n",
    "    res = {'Sequence' : [], 'Clan ID' : [], 'Family ID' : []}\n",
    "    clans = set()\n",
    "    for i in range(len(dataset)):\n",
    "        # Fetch this entry\n",
    "        row = dataset[i]\n",
    "        seq = row[0].astype('int8')\n",
    "        clan = row[2]\n",
    "        family = row[3]\n",
    "\n",
    "        # Check whether this clan is the (num+1)th\n",
    "        clans.add(clan)\n",
    "        if len(clans) > num:\n",
    "            break\n",
    "\n",
    "        # The clan was not the (num+1)th; store its data\n",
    "        res['Sequence'].append(seq)\n",
    "        res['Clan ID'].append(clan)\n",
    "        res['Family ID'].append(family)\n",
    "\n",
    "    return pd.DataFrame.from_dict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Network for Protein Generation\n",
    "This section will define the network architecture for the neural network RNN used as the backbone of the ProWave neural network for protein generation. Note that this network serves as a baseline implementation meant to test our ideas, while the final network we desire to implement, is a WaveNet-based model, which differs significantly from the network below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "clanidsize = 623\n",
    "familyids = 15000\n",
    "\n",
    "class MyRecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRecurrentNet, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=(vocab_size),\n",
    "                         hidden_size=1000,\n",
    "                         num_layers=1,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=1000,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        \n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MyRecurrentNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for us to train our network. In the section below, you will get to put your deep learning skills to use and create your own training loop. You may want to consult previous exercises if you cannot recall how to define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 200\n",
    "\n",
    "# Initialize a new network\n",
    "net = MyRecurrentNet()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a loss function and optimizer for this problem\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.004, momentum=0.6) \n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    net.eval()\n",
    "        \n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        output= net(inputs_one_hot)\n",
    "        batch_loss = criterion(output, targets_idx)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        batch_loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        # Compute loss\n",
    "        # YOUR CODE HERE!\n",
    "        loss = batch_loss\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().numpy()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        output= net(inputs_one_hot)\n",
    "        batch_loss = criterion(output, targets_idx)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        batch_loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        # Compute loss\n",
    "        # YOUR CODE HERE!\n",
    "        loss = batch_loss\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().numpy()\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "        \n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_idx = [word_to_idx[word] for word in targets]\n",
    "\n",
    "# Convert input to tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convert target to tensor\n",
    "targets_idx = torch.LongTensor(targets_idx)\n",
    "\n",
    "# Forward pass\n",
    "outputs = net.forward(inputs_one_hot).data.numpy()\n",
    "\n",
    "print('\\nInput sequence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}