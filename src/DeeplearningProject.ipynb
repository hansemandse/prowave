{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProWave - WaveNet-based Protein Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Hans Jakob Damsgaard & Lucas Balling\n",
    "\n",
    "02456 Deep Learning project: ProGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the commmand below if you have not yet installed the [TAPE project](https://github.com/songlab-cal/tape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tape_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data\n",
    "\n",
    "We were unable to make the data download script, `download_data.sh`, run from Jupyter, so instead we ran it manually and simply placed the resulting files in the right folder for TAPE to find them. We import all the data in the LMDB format as it is most easily worked with in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.datasets import LanguageModelingDataset\n",
    "\n",
    "# Data stored under `<data-path>/data`\n",
    "#data_path = '/Users/lucasballing/Desktop/DeepLearningProject/prowave-main/data/'\n",
    "data_path = 'E:/Pfam/data/'\n",
    "train_data   = LanguageModelingDataset(data_path, 'train')\n",
    "valid_data   = LanguageModelingDataset(data_path, 'valid')\n",
    "holdout_data = LanguageModelingDataset(data_path, 'holdout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding data features\n",
    "\n",
    "To get a good understanding of the data provided in the imported dataset, we provide plots of certain features and their ranges. Data is already split into the three required subsets; train, validation, and holdout by TAPE, so it is also interesting to understand this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has shape (32593668, 4)\n",
      "Validation data has shape (1715454, 4)\n",
      "Holdout data has shape (44311, 4)\n",
      "File data entries look like this: {'primary': 'GCTVEDRCLIGMGAILLNGCVIGSGSLVAAGALITQ', 'protein_length': 36, 'clan': 433, 'family': 9122, 'id': '0'}\n",
      "Encoded data entries look like this: (array([ 2, 11,  7, 23, 25,  9,  8, 21,  7, 15, 13, 11, 16, 11,  5, 13, 15,\n",
      "       15, 17, 11,  7, 25, 13, 11, 22, 11, 22, 15, 25,  5,  5, 11,  5, 15,\n",
      "       13, 23, 20,  3], dtype=int64), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64), 433, 9122)\n"
     ]
    }
   ],
   "source": [
    "# Split sizes\n",
    "print(f'Training data has shape ({len(train_data)}, {len(train_data[0])})')\n",
    "print(f'Validation data has shape ({len(valid_data)}, {len(valid_data[0])})')\n",
    "print(f'Holdout data has shape ({len(holdout_data)}, {len(holdout_data[0])})')\n",
    "\n",
    "# Original data columns\n",
    "from tape.datasets import LMDBDataset\n",
    "lmdb_train = LMDBDataset(data_path+'pfam/pfam_train.lmdb')\n",
    "print(f'File data entries look like this: {lmdb_train[0]}')\n",
    "del lmdb_train\n",
    "\n",
    "# Data columns - all subsets are taken from the same overall dataset, so the columns are the same\n",
    "# From combining information from LMDBDataset and LanguageModelingDataset, we know the columns are\n",
    "# - IUPAC-encoded protein string\n",
    "# - Input mask (for masked-token prediction)\n",
    "# - Protein clan\n",
    "# - Protein family\n",
    "# The protein ID (i.e., its number within its clan and family) is not included\n",
    "print(f'Encoded data entries look like this: {train_data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setify(data, param = 2):\n",
    "#     res = set()\n",
    "#     for i in range(len(data)):\n",
    "#         res.add(data[i][param])\n",
    "#     return res\n",
    "# \n",
    "# # Clans in splits\n",
    "# print(f'Unique clans in training data {len(setify(train_data))}')\n",
    "# print(f'Unique clans in validation data {len(setify(valid_data))}')\n",
    "# print(f'Unique clans in holdout data {len(setify(holdout_data))}')\n",
    "# \n",
    "# # Families in splits\n",
    "# print(f'Unique families in training data {len(setify(train_data, 3))}')\n",
    "# print(f'Unique families in validation data {len(setify(valid_data, 3))}')\n",
    "# print(f'Unique families in holdout data {len(setify(holdout_data, 3))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Dataset 1464\n"
     ]
    }
   ],
   "source": [
    "from tape.datasets import LMDBDataset\n",
    "data_path = '/Users/lucasballing/Desktop/DeepLearningProject/prowave-main/data/pfam/pfam_holdout.lmdb' # 'Your File Path here'\n",
    "\n",
    "\n",
    "np.train_datav2 = np.array(LMDBDataset(data_path))\n",
    "train_datav3 = (LMDBDataset(data_path))\n",
    "print(\"Size of Dataset\",train_data[2][3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Some Histograms\n",
    "This section will plot some histograms of the datasets ot visulise the distribution of Clan and Family ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some large stupid numpy arrays to make it possible to do some histograms\n",
    "np.data = np.zeros((len(holdout_data),2))\n",
    "for x in range(0,len(holdout_data)):\n",
    "    for h in range(2,4):\n",
    "        np.data[x,(h-2)] = holdout_data[x][h]\n",
    "        \n",
    "np.data_train = np.zeros((len(train_data),2))\n",
    "for x in range(0,len(train_data)):\n",
    "    for h in range(2,4):\n",
    "        np.data_train[x,(h-2)] = train_data[x][h]\n",
    "\n",
    "\n",
    "_ = plt.hist(np.data[:,0], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Clan ID\")\n",
    "#plt.Text(0.5, 1.0, \"Histogram with 'auto' bins\")\n",
    "plt.xlabel('Clan ID')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "_ = plt.hist(np.data[:,1], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Family ID\")\n",
    "plt.xlabel('Family ID)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "# Training Set \n",
    "_ = plt.hist(np.data_train[:,0], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Clan ID\")\n",
    "#plt.Text(0.5, 1.0, \"Histogram with 'auto' bins\")\n",
    "plt.xlabel('Clan ID')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "_ = plt.hist(np.data_train[:,1], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Family ID\")\n",
    "plt.xlabel('Family ID)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Import Tokenizers\n",
    "#from .tokenizers import TAPETokenizer\n",
    "\n",
    "# One Hot Encoding the Data\n",
    "aminoacids = 29\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding over Protein Sequence\n",
    "\n",
    "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each Protein Amionacid.\n",
    "\n",
    "| Amionacid    | one-hot encoded vector   |\n",
    "| ------------- |--------------------------|\n",
    "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
    "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
    "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
    "\n",
    "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
    "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
    "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Network for Protein Generation\n",
    "This section will define the network architecture for the neural network RNN used as the backbone of the ProWave neural network for protein generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRecurrentNet(\n",
      "  (lstm): LSTM(29, 50)\n",
      "  (l_out): Linear(in_features=50, out_features=29, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyRecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRecurrentNet, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=aminoacids,\n",
    "                         hidden_size=50,\n",
    "                         num_layers=1,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=50,\n",
    "                            out_features=aminoacids,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MyRecurrentNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
