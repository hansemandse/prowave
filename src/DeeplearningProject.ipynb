{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProWave - WaveNet-based Protein Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Hans Jakob Damsgaard & Lucas Balling\n",
    "\n",
    "02456 Deep Learning project: ProGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the commmand below if you have not yet installed the [TAPE project](https://github.com/songlab-cal/tape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tape_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data\n",
    "\n",
    "We were unable to make the data download script, `download_data.sh`, run from Jupyter, so instead we ran it manually and simply placed the resulting files in the right folder for TAPE to find them. We import all the data in the LMDB format as it is most easily worked with in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.datasets import LanguageModelingDataset\n",
    "\n",
    "# Data stored under `<data-path>/data`\n",
    "data_path = '/Users/lucasballing/Desktop/DeepLearningProject/prowave-main/data/'\n",
    "#data_path = 'E:/Pfam/data/'\n",
    "train_data   = LanguageModelingDataset(data_path, 'train')\n",
    "valid_data   = LanguageModelingDataset(data_path, 'valid')\n",
    "holdout_data = LanguageModelingDataset(data_path, 'holdout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding data features\n",
    "\n",
    "To get a good understanding of the data provided in the imported dataset, we provide plots of certain features and their ranges. Data is already split into the three required subsets; train, validation, and holdout by TAPE, so it is also interesting to understand this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has shape (32593668, 4)\n",
      "Validation data has shape (1715454, 4)\n",
      "Holdout data has shape (44311, 4)\n",
      "File data entries look like this: {'primary': 'GCTVEDRCLIGMGAILLNGCVIGSGSLVAAGALITQ', 'protein_length': 36, 'clan': 433, 'family': 9122, 'id': '0'}\n",
      "Encoded data entries look like this: (array([ 2, 11,  7, 23, 25,  9,  8, 21,  7, 15, 13, 11, 16, 11,  5, 13, 15,\n",
      "       15, 17, 11,  7, 25, 13, 11, 22, 11, 22, 15, 25,  5,  5, 11,  5, 15,\n",
      "       13, 23, 20,  3]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 433, 9122)\n"
     ]
    }
   ],
   "source": [
    "# Split sizes\n",
    "print(f'Training data has shape ({len(train_data)}, {len(train_data[0])})')\n",
    "print(f'Validation data has shape ({len(valid_data)}, {len(valid_data[0])})')\n",
    "print(f'Holdout data has shape ({len(holdout_data)}, {len(holdout_data[0])})')\n",
    "\n",
    "# Original data columns\n",
    "from tape.datasets import LMDBDataset\n",
    "lmdb_train = LMDBDataset(data_path+'pfam/pfam_train.lmdb')\n",
    "print(f'File data entries look like this: {lmdb_train[0]}')\n",
    "del lmdb_train\n",
    "\n",
    "# Data columns - all subsets are taken from the same overall dataset, so the columns are the same\n",
    "# From combining information from LMDBDataset and LanguageModelingDataset, we know the columns are\n",
    "# - IUPAC-encoded protein string\n",
    "# - Input mask (for masked-token prediction)\n",
    "# - Protein clan\n",
    "# - Protein family\n",
    "# The protein ID (i.e., its number within its clan and family) is not included\n",
    "print(f'Encoded data entries look like this: {train_data[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setify(data, param = 2):\n",
    "#     res = set()\n",
    "#     for i in range(len(data)):\n",
    "#         res.add(data[i][param])\n",
    "#     return res\n",
    "# \n",
    "# # Clans in splits\n",
    "# print(f'Unique clans in training data {len(setify(train_data))}')\n",
    "# print(f'Unique clans in validation data {len(setify(valid_data))}')\n",
    "# print(f'Unique clans in holdout data {len(setify(holdout_data))}')\n",
    "# \n",
    "# # Families in splits\n",
    "# print(f'Unique families in training data {len(setify(train_data, 3))}')\n",
    "# print(f'Unique families in validation data {len(setify(valid_data, 3))}')\n",
    "# print(f'Unique families in holdout data {len(setify(holdout_data, 3))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "from tape.datasets import LMDBDataset\n",
    "data_path = '/Users/lucasballing/Desktop/DeepLearningProject/prowave-main/data/pfam/pfam_holdout.lmdb' # 'Your File Path here'\n",
    "\n",
    "\n",
    "#np.train_datav2 = np.array(LMDBDataset(data_path))\n",
    "#train_datav3 = (LMDBDataset(data_path))\n",
    "#print(\"Size of Dataset\",train_data[2][3] )\n",
    "data = np.asarray(holdout_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Some Histograms\n",
    "This section will plot some histograms of the datasets ot visulise the distribution of Clan and Family ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b677c28e9a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# arguments are passed to np.histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Proteins - Clan ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#plt.Text(0.5, 1.0, \"Histogram with 'auto' bins\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vertical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m         label=None, stacked=False, *, data=None, **kwargs):\n\u001b[0;32m-> 2669\u001b[0;31m     return gca().hist(\n\u001b[0m\u001b[1;32m   2670\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m         \u001b[0mcumulative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcumulative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6713\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6714\u001b[0m                     \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6715\u001b[0;31m                 patch = _barfunc(bins[:-1]+boffset, height, width,\n\u001b[0m\u001b[1;32m   6716\u001b[0m                                  \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6717\u001b[0m                                  color=c, **{bottom_kwarg: bottom})\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m             r = mpatches.Rectangle(\n\u001b[0m\u001b[1;32m   2482\u001b[0m                 \u001b[0mxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                 \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xy, width, height, angle, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \"\"\"\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mPatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, edgecolor, facecolor, color, linewidth, linestyle, antialiased, hatch, fill, capstyle, joinstyle, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_linestyle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinestyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_linewidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mset_fill\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[1;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_facecolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_edgecolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m_set_facecolor\u001b[0;34m(self, color)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patch.facecolor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_facecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrgba\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Suppress exception chaining of cache lookup failure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Checks that don't work: `map(float, ...)`, `np.array(..., float)` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# `np.array(...).astype(float)` would all convert \"0.5\" to 0.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Checks that don't work: `map(float, ...)`, `np.array(..., float)` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# `np.array(...).astype(float)` would all convert \"0.5\" to 0.5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating some large stupid numpy arrays to make it possible to do some histograms\n",
    "#data = np.zeros((len(holdout_data),2))\n",
    "#for x in range(0,len(holdout_data)):\n",
    "#    for h in range(2,4):\n",
    "#        data[x,(h-2)] = holdout_data[x][h]\n",
    "#print(\"Converted the Holdout dataset to numpy\")\n",
    "\n",
    "#data_valid = np.zeros((len(valid_data),2))\n",
    "#for x in range(0,len(valid_data)):\n",
    "##    for h in range(2,4):\n",
    "#        data_valid[x,(h-2)] = valid_data[x][h]\n",
    "         \n",
    "#print(\"Converted the Validation dataset to numpy\")\n",
    "\n",
    "\n",
    "_ = plt.hist(data[:,0], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Clan ID\")\n",
    "#plt.Text(0.5, 1.0, \"Histogram with 'auto' bins\")\n",
    "plt.xlabel('Clan ID')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "_ = plt.hist(data[:,1], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Family ID\")\n",
    "plt.xlabel('Family ID)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "# Training Set \n",
    "_ = plt.hist(data_valid[:,0], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Clan ID\")\n",
    "#plt.Text(0.5, 1.0, \"Histogram with 'auto' bins\")\n",
    "plt.xlabel('Clan ID')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "_ = plt.hist(data_valid[:,1], bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Proteins - Family ID\")\n",
    "plt.xlabel('Family ID)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Import Tokenizers\n",
    "#from .tokenizers import TAPETokenizer\n",
    "\n",
    "# One Hot Encoding the Data\n",
    "aminoacids = 29\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding over Protein Sequence\n",
    "\n",
    "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each Protein Amionacid.\n",
    "\n",
    "| Amionacid    | one-hot encoded vector   |\n",
    "| ------------- |--------------------------|\n",
    "| Ala = A        | $= [1, 0, 0, \\ldots, 0]$ |\n",
    "| Asx = B        | $= [0, 1, 0, \\ldots, 0]$ |\n",
    "| Cys = C    | $= [0, 0, 1, \\ldots, 0]$ |\n",
    "| ... | ... |\n",
    "| pad   | $= [0, 0, 0, \\ldots, 1]$ |\n",
    "\n",
    "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
    "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
    "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our one-hot encoding of 'a' has shape (30,).\n",
      "Our one-hot encoding of 'a b' has shape (203, 30, 1).\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "[ 2 20 13 11  5 11 11 23 15 15 21 17 28  8 10 22 19  8 11  7  9 11 23 13\n",
      " 25 22 22 12 10 15 21 19 25 13 11 16 20  8  5  5 26 11 15 15  8 11 16 17\n",
      "  8  5 11 15  5 25 22 15 23 10 11 11 21 10 25 12 11 19 11 10  5 25 15 13\n",
      " 25 15 21 28 15 15  9 23  7 23 23 25 11  9  5 25 11 14 15 14 22 15 19 13\n",
      "  5 13 19 20 17 25 23 15 25  8 19  9 21 22 15 23 25 10 25 11 19  8 13 19\n",
      " 15 23  9  5 19  8  5  7  5  5 17 12 20 12 15 19 25 19  8  9 20  9 20 22\n",
      " 22 21 23 20  9 21 15 22  5 13 21  5  5 11 23  8 25  5  5 16 15  9 19 19\n",
      " 15 28 20 23  5 28  8  9 26 15 11 23 15 28 23  5 12 28 21 19 22  9 11 21\n",
      " 25 23 28 28 26 19 11  8 22 26  3]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word, vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "test_word = one_hot_encode(1, vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(holdout_data[1][0], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')\n",
    "print(test_sentence)\n",
    "print(holdout_data[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Network for Protein Generation\n",
    "This section will define the network architecture for the neural network RNN used as the backbone of the ProWave neural network for protein generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyRecurrentNet(\n",
      "  (lstm): LSTM(30, 50)\n",
      "  (l_out): Linear(in_features=50, out_features=30, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "clanidsize = 623\n",
    "familyids = 15000\n",
    "\n",
    "class MyRecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRecurrentNet, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=(vocab_size),\n",
    "                         hidden_size=1000,\n",
    "                         num_layers=1,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=1000,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        \n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MyRecurrentNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for us to train our network. In the section below, you will get to put your deep learning skills to use and create your own training loop. You may want to consult previous exercises if you cannot recall how to define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-a8be8e7ccba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# For each sentence in validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# One-hot encode input and target sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_set' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 200\n",
    "\n",
    "# Initialize a new network\n",
    "net = MyRecurrentNet()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a loss function and optimizer for this problem\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.004, momentum=0.6) \n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    net.eval()\n",
    "        \n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        output= net(inputs_one_hot)\n",
    "        batch_loss = criterion(output, targets_idx)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        batch_loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        # Compute loss\n",
    "        # YOUR CODE HERE!\n",
    "        loss = batch_loss\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().numpy()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        output= net(inputs_one_hot)\n",
    "        batch_loss = criterion(output, targets_idx)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        batch_loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        # Compute loss\n",
    "        # YOUR CODE HERE!\n",
    "        loss = batch_loss\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().numpy()\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "        \n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_idx = [word_to_idx[word] for word in targets]\n",
    "\n",
    "# Convert input to tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convert target to tensor\n",
    "targets_idx = torch.LongTensor(targets_idx)\n",
    "\n",
    "# Forward pass\n",
    "outputs = net.forward(inputs_one_hot).data.numpy()\n",
    "\n",
    "print('\\nInput sequence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
