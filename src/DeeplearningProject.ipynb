{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProWave - WaveNet-based Protein Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Hans Jakob Damsgaard & Lucas Balling\n",
    "\n",
    "02456 Deep Learning project: ProGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the commmand below if you have not yet installed the [TAPE project](https://github.com/songlab-cal/tape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tape_proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data\n",
    "\n",
    "We were unable to make the data download script, `download_data.sh`, run from Jupyter, so instead we ran it manually and simply placed the resulting files in the right folder for TAPE to find them. We import all the data in the LMDB format as it is most easily worked with in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.datasets import LanguageModelingDataset\n",
    "ModelPath = './Pretrained/'\n",
    "#data_path = '/Users/lucasballing/Desktop/DeepLearningProject/prowave-main/data/pfam/'\n",
    "data_path = 'E:/Pfam/data/'\n",
    "train_data   = LanguageModelingDataset(data_path, 'train')\n",
    "valid_data   = LanguageModelingDataset(data_path, 'valid')\n",
    "holdout_data = LanguageModelingDataset(data_path, 'holdout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding data features\n",
    "\n",
    "To get a good understanding of the data provided in the imported dataset, we provide plots of certain features and their ranges. Data is already split into the three required subsets; train, validation, and holdout by TAPE, so it is also interesting to understand this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sizes\n",
    "print(f'Training data has shape ({len(train_data)}, {len(train_data[0])})')\n",
    "print(f'Validation data has shape ({len(valid_data)}, {len(valid_data[0])})')\n",
    "print(f'Holdout data has shape ({len(holdout_data)}, {len(holdout_data[0])})')\n",
    "\n",
    "# Original data columns\n",
    "from tape.datasets import LMDBDataset\n",
    "lmdb_train = LMDBDataset(data_path+'pfam/pfam_train.lmdb')\n",
    "print(f'File data entries look like this: {lmdb_train[0]}')\n",
    "del lmdb_train\n",
    "\n",
    "# Data columns - all subsets are taken from the same overall dataset, so the columns are the same\n",
    "# From combining information from LMDBDataset and LanguageModelingDataset, we know the columns are\n",
    "# - IUPAC-encoded protein string\n",
    "# - Input mask (for masked-token prediction)\n",
    "# - Protein clan\n",
    "# - Protein family\n",
    "# The protein ID (i.e., its number within its clan and family) is not included\n",
    "print(f'Encoded data entries look like this: {train_data[0]}')\n",
    "del train_data\n",
    "del valid_data\n",
    "del holdout_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import setify\n",
    "\n",
    "# Fetch results from all splits\n",
    "#results = setify([train_data, valid_data, holdout_data])\n",
    "\n",
    "# Clans in splits\n",
    "#clans = results[0][0]\n",
    "#print(f'Unique clans in training data {len(clans[0])}')\n",
    "#print(f'Unique clans in validation data {len(clans[1])}')\n",
    "#print(f'Unique clans in holdout data {len(clans[2])}')\n",
    "\n",
    "# Families in splits\n",
    "#families = results[0][1]\n",
    "#print(f'Unique families in training data {len(families[0])}')\n",
    "#print(f'Unique families in validation data {len(families[1])}')\n",
    "#print(f'Unique families in holdout data {len(families[2])}')\n",
    "\n",
    "# PRINTS:\n",
    "# Unique clans in training data 623\n",
    "# Unique clans in validation data 623\n",
    "# Unique clans in holdout data 8\n",
    "# Unique families in training data 17737\n",
    "# Unique families in validation data 15974\n",
    "# Unique families in holdout data 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea and Initial Implementation\n",
    "We intend to follow the ideas presented in the ProGen paper relatively closely. That is, our network will be trained on conditioned aminoacid sequences with the only two available conditioning tags being the clan and family IDs. To enable this, our network's input will be as shown on the figure below.\n",
    "\n",
    "<img src=\"../Training.png\" width=\"500\"/>\n",
    "\n",
    "So, like ProGen, our input $x=[c;a]$ is a sequence of encoded conditioning tags $c=(c_0,...,c_n)$ (in this case just two), and a starting sequence $a=(a_0, ..., a_n)$ of aminoacids for starting the sequence generation. The sequence is fed one character at a time to the network, which accumulates state before starting generation. We intend to let the network run free either until it generates an end character or until the generated sequence has length 500.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "One way to represent a fixed amount of words is through one-hot encoding. We intend to use one-hot encoding for both aminoacids as well as conditioning tags meaning that the input characters to our network become rather large. This may seem inefficient due to the great sparsity in the vectors produced, but we intend to limit this sparsity by decreasing the number of clans and families considered. This will allow us to generate proteins only for a subset of the available clans and families but at a much shorter required training and evaluation time. \n",
    "\n",
    "An example of a one-hot encoding is shown below:\n",
    "\n",
    "| Amionacid    | one-hot encoded vector   |\n",
    "| ------------- |--------------------------|\n",
    "| pad   | $= [0, 0, 0, \\ldots, 0]$ |\n",
    "| CLS   | $= [0, 1, 0, \\ldots, 0]$ |\n",
    "| SEP   | $= [0, 0, 1, \\ldots, 0]$ | \n",
    "| Ala = A  | $= [0, 0, 0, 1, 0, \\ldots, 0]$ |\n",
    "| Asx = B  | $= [0, 0, 0, 0, 1,\\ldots, 0]$ |\n",
    "| ... | ... |\n",
    "\n",
    "This notebook will as tape use the BERT sequence  [CLS] X [SEP] - Start and stop sequences.\n",
    "\n",
    "Although the implementation from earlier in the course, which is used below, works well, we have decided to instead use Pytorch's own Embedding module which allows us to skip manually encoding vectors.\n",
    "\n",
    "For more information about the one hot incoding see the TAPE: https://github.com/songlab-cal/tape/blob/master/tape/tokenizers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import one_hot_encode, one_hot_encode_sequence\n",
    "\n",
    "# Testing the implementation\n",
    "VOCAB_SIZE = 30\n",
    "#test_word = one_hot_encode(1,VOCAB_SIZE)\n",
    "#print(f'Our one-hot encoding of \\'1\\' has shape {test_word.shape}.')\n",
    "#print(test_word)\n",
    "\n",
    "#test_sentence = one_hot_encode_sequence(holdout_data[1][0], VOCAB_SIZE)\n",
    "#print(f'Our one-hot encoding of \\'{holdout_data[1][0]}\\' has shape {test_sentence.shape}.')\n",
    "#print(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing input size\n",
    "To reduce training time and problem size, we intend to extract 10 clans and their families from the training set and use those for training the network. Data is stored in sorted order in the training data set meaning that we can simply store elements until we see the 11th clan ID at which we can stop looking through the dataset. This is preferable over having to look through the entire dataset (as we did to gather plot data earlier), as such an operation takes a long time due to the mere size of the dataset.\n",
    "\n",
    "As part of this operation, we also throw away the input masks that are included in the original dataset and attempt to limit data size by using `int8` instead of the original `int64` format for encoded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLANS = 10\n",
    "FAMILIES = 100\n",
    "LENGTH = 512\n",
    "LENGTH_OUT = 514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data size by picking only 10 first clans\n",
    "#from utils import get_data\n",
    "#for d, n in zip([train_data, valid_data, holdout_data], ['train', 'valid', 'holdout']):\n",
    "#    dataset = get_data(d, CLANS)\n",
    "#    with open(data_path+n+'_red.pkl', 'wb') as f:\n",
    "#        pkl.dump(dataset, f)\n",
    "#    del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce data size by picking only the 100 first families\n",
    "#for i, o in zip(['train_red.pkl', 'valid_red.pkl', 'holdout_red.pkl'], ['train_red2.pkl', 'valid_red2.pkl', #'holdout_red2.pkl']):\n",
    "#    # Load reduced dataset\n",
    "#    with open(data_path+i, 'rb') as fin:\n",
    "#        dataset = pkl.load(fin)\n",
    "#    # Reduce number of families\n",
    "#    dataset = dataset[dataset['Family ID'].isin(list(range(FAMILIES)))]\n",
    "#    # Store reduced data\n",
    "#    with open(data_path+o, 'wb') as fout:\n",
    "#        pkl.dump(dataset, fout)\n",
    "#    del dataset\n",
    "\n",
    "# Use the reduced dataset\n",
    "with open(data_path+'train_red2.pkl', 'rb') as f:\n",
    "    train_data = pkl.load(f)\n",
    "with open(data_path+'valid_red2.pkl', 'rb') as f:\n",
    "    valid_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code reduces the overall data size to around 50 MB training data, 2.5 MB validation data, and unfortunately no holdout data. We shall now plot some characteristics of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of proteins in each clan\n",
    "pdf = pd.DataFrame.from_dict({'Clan ID' : list(range(CLANS)),'Count' : [len(train_data[train_data['Clan ID']==i]) for i in range(CLANS)]})\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x='Clan ID', y='Count', data=pdf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of families in each clan\n",
    "pdf = pd.DataFrame.from_dict({'Clan ID' : list(range(CLANS)),'Count' : [len(set(train_data[train_data['Clan ID']==i]['Family ID'])) for i in range(CLANS)]})\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x='Clan ID', y='Count', data=pdf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of proteins in each family\n",
    "pdf = pd.DataFrame.from_dict({'Family ID' : list(range(FAMILIES)),'Count' : [len(train_data[train_data['Family ID']==i]) for i in range(FAMILIES)]})\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.barplot(x='Family ID', y='Count', data=pdf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the data\n",
    "print(train_data.info())\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And repeat this for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of proteins in each clan\n",
    "pdf = pd.DataFrame.from_dict({'Clan ID' : list(range(CLANS)),'Count' : [len(valid_data[valid_data['Clan ID']==i]) for i in range(CLANS)]})\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x='Clan ID', y='Count', data=pdf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of proteins in each clan\n",
    "pdf = pd.DataFrame.from_dict({'Clan ID' : list(range(CLANS)),'Count' : [len(valid_data[valid_data['Clan ID']==i]) for i in range(CLANS)]})\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x='Clan ID', y='Count', data=pdf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of proteins in each family\n",
    "pdf = pd.DataFrame.from_dict({'Family ID' : list(range(FAMILIES)),'Count' : [len(valid_data[valid_data['Family ID']==i]) for i in range(FAMILIES)]})\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.barplot(x='Family ID', y='Count', data=pdf)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the data\n",
    "print(valid_data.info())\n",
    "print(valid_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution of clans and families is reasonably even across both datasets (as expected). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Getting the Data ready for utilization. This covers changing the Clan IDs, Family IDs and aminoacid sequences into values from 0 to 139 to prepare them for embedding. We place the vocabulary of aminoacids in the beginning of this encoding as their values then correspond directly to their indices in the vocabulary used. This means that all Clan IDs will have an offset of 30, and Family IDs will have an offset of 40. \n",
    "\n",
    "Here, the value zero will be used for padding purposes.\n",
    "\n",
    "### Input\n",
    "\n",
    "| Feature   | Interval of Values   |\n",
    "| ------------- |--------------------------|\n",
    "| Aminoacid vocabulary (30) | $= 0 \\ldots 29 $ |\n",
    "| Clan ID (10)    | $= 30 \\ldots 39 $ |\n",
    "| Family ID (100)    | $= 40 \\ldots 139 $ |\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "| Feature   | Interval of Values   |\n",
    "| ------------- |--------------------------|\n",
    "| Aminoacid vocabulary (30) | $= 0 \\ldots 29 $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine clan and family offsets\n",
    "offset_Clan = VOCAB_SIZE\n",
    "offset_Family = VOCAB_SIZE + CLANS\n",
    "\n",
    "# Map offsets onto clan and family IDs\n",
    "train_data['Clan ID'] += offset_Clan\n",
    "train_data['Family ID'] += offset_Family\n",
    "valid_data['Clan ID'] += offset_Clan\n",
    "valid_data['Family ID'] += offset_Family\n",
    "\n",
    "# Print some heads\n",
    "print(train_data.head())\n",
    "print(valid_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad all the Inputs and Outputs\n",
    "Creating Input data and target values with padding - inspired by [this guide](https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e). This will make all inputs and outputs the same size. Currently, there is an input X and an output Y, which both need some padding before they can be used for training of the LSTM and the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function\n",
    "from utils import get_data_input\n",
    "\n",
    "# Padding the input sequence - Clan ID , Family ID and aminoacid sequence (512 Bytes) with padding if less than 512 long.\n",
    "X_train = get_data_input(train_data, LENGTH)\n",
    "X_valid = get_data_input(valid_data, LENGTH)\n",
    "\n",
    "# Print some heads\n",
    "print(X_train.head())\n",
    "print(X_valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function\n",
    "from utils import get_data_output\n",
    "\n",
    "# Padding the output sequence\n",
    "Y_train = get_data_output(train_data, LENGTH_OUT)\n",
    "Y_valid = get_data_output(valid_data, LENGTH_OUT)\n",
    "\n",
    "# Print some heads\n",
    "print(Y_train.head())\n",
    "print(Y_valid.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a DataLoader to make training more efficient. Unfortunately, Pickle loads the data as an implicit `object_` type (which Pandas also uses to represent DataFrame entries that are not scalars or similar basic types), which must be converted to a numeric type that PyTorch can work with. This is done with the conversion to a Python list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 50\n",
    "TRAIN_SIZE = 100 # max len(X_train.values)\n",
    "VALID_SIZE = 2000  # max len(X_valid.values)\n",
    "\n",
    "# Transform training data first\n",
    "d_train = torch.tensor([pd.to_numeric(x[0]) for x in X_train.values[:TRAIN_SIZE]]).to(torch.long)\n",
    "d_targets = torch.tensor([pd.to_numeric(y[0]) for y in Y_train.values[:TRAIN_SIZE]]).to(torch.long)\n",
    "train_dataset = TensorDataset(d_train, d_targets)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 1)\n",
    "\n",
    "# Then transform validation data\n",
    "v_train = torch.tensor([pd.to_numeric(x[0]) for x in X_valid.values[:VALID_SIZE]]).to(torch.long)\n",
    "v_targets = torch.tensor([pd.to_numeric(y[0]) for y in Y_valid.values[:VALID_SIZE]]).to(torch.long)\n",
    "valid_dataset = TensorDataset(v_train, v_targets)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Network for Protein Generation\n",
    "This section will define the network architecture for the neural network RNN used as the backbone of the ProWave neural network for protein generation. Note that this network serves as a baseline implementation meant to test our ideas, while the final network we desire to implement, is a WaveNet-based model, which differs significantly from the network below.\n",
    "\n",
    "In the following, we consider the so-called validation set as our test set and simply ignore validating the model within each epoch.\n",
    "\n",
    "We first define an LSTM-based model - heavily inspired by [this guide](https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ProLSTM, train\n",
    "# LSTM network\n",
    "netProLSTM = ProLSTM(batch_size=BATCH_SIZE)\n",
    "netProLSTM = train(netProLSTM, train_loader, epochs = EPOCHS)\n",
    "\n",
    "# Save LSTM Model\n",
    "#with open(ModelPath + '/netProLSTM_30Epochs_v5', 'wb') as f:\n",
    "#    torch.save(netProLSTM.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a GRU-based model (essentially the same as the LSTM, but with GRU cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ProGRU, train\n",
    "# GRU network\n",
    "netProGRU = ProGRU(batch_size=BATCH_SIZE)\n",
    "netProGRU = train(netProGRU, train_loader, epochs = EPOCHS)\n",
    "\n",
    "# Save GRU Model\n",
    "#with open(ModelPath + '/netProGRU_30Epochs_v5', 'wb') as f:\n",
    "#    torch.save(netProLSTM.state_dict(), f)"
   ]
  },
  {
   "source": [
    "## Transformer for Protein Generation\n",
    "Next, we will define a Transformer model. We define the model based on the tutorial available directly from PyTorch [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html). \n",
    "\n",
    "We use a the template model from the tutorial with few changes to make it match our other networks in size. Some of the savings arise from simply reducing the input and output vocabulary sizes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import ProTrans, train\n",
    "# Transformer network\n",
    "netTrans = ProTrans(nintoken = 140, nouttoken = 30, ninp = 32, nhid = 64, nlayers = 2, nhead = 2, dropout = 0.2)\n",
    "netTrans = train(netTrans, train_loader, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet for Protein Generation aka ProWave\n",
    "We now continue our exploration by defining a WaveNet model - heavily inspired by [this guide](https://github.com/Dankrushen/Wavenet-PyTorch).\n",
    "\n",
    "We aim to make this model have approximately the same number of weights as the previous models in order to better compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ProWaveNet, nll_loss\n",
    "# WaveNet network\n",
    "netProWave = ProWaveNet(LENGTH_OUT, num_layers = 8, num_hidden = 32)\n",
    "print(netProWave)\n",
    "netProWave.criterion = nll_loss\n",
    "netProWave.optimizer = optim.Adam(netProWave.parameters(), lr=0.00086)\n",
    "netProWave.scheduler = optim.lr_scheduler.StepLR(netProWave.optimizer, step_size = 10, gamma = 0.5)\n",
    "netProWave.train(train_loader, num_epochs = EPOCHS, disp_interval = 1)\n",
    "# Generator\n",
    "netProGen = Generator(netProWave, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}